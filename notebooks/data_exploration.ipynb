{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bfce1c",
   "metadata": {},
   "source": [
    "# Bangla Punctuation Restoration - Data Exploration\n",
    "\n",
    "This notebook explores the dataset for Bangla punctuation restoration, analyzing text patterns, punctuation distribution, and data quality metrics.\n",
    "\n",
    "## Objectives:\n",
    "1. Load and explore the Bangla punctuation dataset\n",
    "2. Analyze punctuation patterns and distribution\n",
    "3. Examine text statistics and characteristics\n",
    "4. Evaluate dataset quality\n",
    "5. Explore adversarial examples\n",
    "6. Visualize key insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee87859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.data.dataset_loader import BanglaDatasetLoader\n",
    "from src.data.data_processor import BanglaTextProcessor\n",
    "from src.data.adversarial_attacks import AdversarialAttacks\n",
    "from config import Config\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Imports loaded successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55bc451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration and data loader\n",
    "config = Config()\n",
    "loader = BanglaDatasetLoader(config)\n",
    "processor = BanglaTextProcessor()\n",
    "\n",
    "print(\"üîÑ Loading Bangla punctuation dataset...\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    # Try to load from existing sources or generate sample data\n",
    "    dataset = loader.load_dataset()\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {len(dataset)} samples\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nüìù Sample data:\")\n",
    "    for i, (original, punctuated) in enumerate(dataset[:3]):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Original: {original}\")\n",
    "        print(f\"  Punctuated: {punctuated}\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error loading dataset: {e}\")\n",
    "    print(\"üîÑ Generating sample dataset for exploration...\")\n",
    "    \n",
    "    # Generate sample data for demonstration\n",
    "    sample_texts = [\n",
    "        (\"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶•‡¶æ‡¶ï‡¶ø\", \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶•‡¶æ‡¶ï‡¶ø‡•§\"),\n",
    "        (\"‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶Ü‡¶õ‡ßã\", \"‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶Ü‡¶õ‡ßã?\"),\n",
    "        (\"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶≠‡¶æ‡¶≤‡ßã\", \"‡¶Ü‡¶ú ‡¶Ü‡¶¨‡¶π‡¶æ‡¶ì‡¶Ø‡¶º‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶≠‡¶æ‡¶≤‡ßã‡•§\"),\n",
    "        (\"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶§ ‡¶ñ‡¶æ‡¶á ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá ‡¶¶‡ßÅ‡¶™‡ßÅ‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∞‡¶æ‡¶§‡ßá\", \"‡¶Ü‡¶Æ‡¶ø ‡¶≠‡¶æ‡¶§ ‡¶ñ‡¶æ‡¶á ‡¶∏‡¶ï‡¶æ‡¶≤‡ßá, ‡¶¶‡ßÅ‡¶™‡ßÅ‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∞‡¶æ‡¶§‡ßá‡•§\"),\n",
    "        (\"‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶∏‡¶¨‡ßá‡¶®\", \"‡¶Ü‡¶™‡¶®‡¶ø ‡¶ï‡¶ø ‡¶Ü‡¶∏‡¶¨‡ßá‡¶®?\"),\n",
    "    ]\n",
    "    dataset = sample_texts * 20  # Replicate for analysis\n",
    "    print(f\"‚úÖ Sample dataset generated with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d13a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Dataset Statistics\n",
    "print(\"üìä BASIC DATASET STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert to lists for easier analysis\n",
    "original_texts = [item[0] for item in dataset]\n",
    "punctuated_texts = [item[1] for item in dataset]\n",
    "\n",
    "# Text length statistics\n",
    "original_lengths = [len(text) for text in original_texts]\n",
    "punctuated_lengths = [len(text) for text in punctuated_texts]\n",
    "word_counts = [len(text.split()) for text in original_texts]\n",
    "\n",
    "stats_df = pd.DataFrame({\n",
    "    'Metric': ['Total Samples', 'Avg Original Length', 'Avg Punctuated Length', \n",
    "               'Avg Word Count', 'Min Length', 'Max Length'],\n",
    "    'Value': [\n",
    "        len(dataset),\n",
    "        np.mean(original_lengths),\n",
    "        np.mean(punctuated_lengths),\n",
    "        np.mean(word_counts),\n",
    "        min(original_lengths),\n",
    "        max(original_lengths)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# Create distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0, 0].hist(original_lengths, bins=20, alpha=0.7, label='Original', color='skyblue')\n",
    "axes[0, 0].hist(punctuated_lengths, bins=20, alpha=0.7, label='Punctuated', color='orange')\n",
    "axes[0, 0].set_xlabel('Text Length (characters)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Text Length Distribution')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(word_counts, bins=15, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Word Count Distribution')\n",
    "\n",
    "# Length difference\n",
    "length_diff = [p - o for o, p in zip(original_lengths, punctuated_lengths)]\n",
    "axes[1, 0].hist(length_diff, bins=15, alpha=0.7, color='coral')\n",
    "axes[1, 0].set_xlabel('Length Difference (Punctuated - Original)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Punctuation Addition Impact')\n",
    "\n",
    "# Box plot for length comparison\n",
    "axes[1, 1].boxplot([original_lengths, punctuated_lengths], \n",
    "                   labels=['Original', 'Punctuated'])\n",
    "axes[1, 1].set_ylabel('Text Length')\n",
    "axes[1, 1].set_title('Length Comparison Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557501ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation Pattern Analysis\n",
    "print(\"üîç PUNCTUATION PATTERN ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define Bangla punctuation marks\n",
    "bangla_punctuation = ['‡•§', '?', '!', ',', ';', ':', '\"', \"'\", '-', '‚Äî', '(', ')', '[', ']']\n",
    "english_punctuation = ['.', '?', '!', ',', ';', ':', '\"', \"'\", '-', '‚Äî', '(', ')', '[', ']']\n",
    "all_punctuation = bangla_punctuation + english_punctuation\n",
    "\n",
    "# Count punctuation occurrences\n",
    "punctuation_counts = Counter()\n",
    "for text in punctuated_texts:\n",
    "    for char in text:\n",
    "        if char in all_punctuation:\n",
    "            punctuation_counts[char] += 1\n",
    "\n",
    "print(\"üî§ Punctuation frequency:\")\n",
    "for punct, count in punctuation_counts.most_common():\n",
    "    percentage = (count / sum(punctuation_counts.values())) * 100\n",
    "    print(f\"  '{punct}': {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Analyze punctuation positions\n",
    "def analyze_punctuation_positions(texts):\n",
    "    positions = {'beginning': 0, 'middle': 0, 'end': 0}\n",
    "    for text in texts:\n",
    "        for i, char in enumerate(text):\n",
    "            if char in all_punctuation:\n",
    "                if i == 0:\n",
    "                    positions['beginning'] += 1\n",
    "                elif i == len(text) - 1:\n",
    "                    positions['end'] += 1\n",
    "                else:\n",
    "                    positions['middle'] += 1\n",
    "    return positions\n",
    "\n",
    "punct_positions = analyze_punctuation_positions(punctuated_texts)\n",
    "print(f\"\\nüìç Punctuation positions:\")\n",
    "for pos, count in punct_positions.items():\n",
    "    print(f\"  {pos.capitalize()}: {count}\")\n",
    "\n",
    "# Visualize punctuation analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Punctuation frequency bar plot\n",
    "punct_chars = list(punctuation_counts.keys())\n",
    "punct_freq = list(punctuation_counts.values())\n",
    "axes[0, 0].bar(punct_chars, punct_freq, color='lightcoral')\n",
    "axes[0, 0].set_xlabel('Punctuation Marks')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Punctuation Mark Frequency')\n",
    "\n",
    "# Punctuation position pie chart\n",
    "axes[0, 1].pie(punct_positions.values(), labels=punct_positions.keys(), \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Punctuation Position Distribution')\n",
    "\n",
    "# Sentences ending with different punctuation\n",
    "ending_punct = Counter()\n",
    "for text in punctuated_texts:\n",
    "    if text and text[-1] in all_punctuation:\n",
    "        ending_punct[text[-1]] += 1\n",
    "\n",
    "if ending_punct:\n",
    "    axes[1, 0].bar(ending_punct.keys(), ending_punct.values(), color='lightblue')\n",
    "    axes[1, 0].set_xlabel('Ending Punctuation')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].set_title('Sentence Ending Punctuation')\n",
    "\n",
    "# Text without punctuation vs with punctuation\n",
    "no_punct_count = sum(1 for text in original_texts if not any(p in text for p in all_punctuation))\n",
    "with_punct_count = len(original_texts) - no_punct_count\n",
    "axes[1, 1].pie([no_punct_count, with_punct_count], \n",
    "               labels=['No Punctuation', 'Has Punctuation'], \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 1].set_title('Original Text Punctuation Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3263a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bangla Text Characteristics Analysis\n",
    "print(\"üî§ BANGLA TEXT CHARACTERISTICS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define Bangla character ranges\n",
    "bangla_chars = set()\n",
    "for text in original_texts:\n",
    "    for char in text:\n",
    "        if '\\u0980' <= char <= '\\u09FF':  # Bangla Unicode range\n",
    "            bangla_chars.add(char)\n",
    "\n",
    "print(f\"üìù Unique Bangla characters found: {len(bangla_chars)}\")\n",
    "print(f\"üî§ Characters: {sorted(bangla_chars)[:20]}...\")  # Show first 20\n",
    "\n",
    "# Analyze character frequency\n",
    "char_counter = Counter()\n",
    "for text in original_texts:\n",
    "    for char in text:\n",
    "        if '\\u0980' <= char <= '\\u09FF':\n",
    "            char_counter[char] += 1\n",
    "\n",
    "print(f\"\\nüìä Top 10 most frequent Bangla characters:\")\n",
    "for char, count in char_counter.most_common(10):\n",
    "    print(f\"  '{char}': {count}\")\n",
    "\n",
    "# Word analysis\n",
    "all_words = []\n",
    "for text in original_texts:\n",
    "    words = text.split()\n",
    "    all_words.extend(words)\n",
    "\n",
    "word_counter = Counter(all_words)\n",
    "print(f\"\\nüìö Vocabulary statistics:\")\n",
    "print(f\"  Total words: {len(all_words)}\")\n",
    "print(f\"  Unique words: {len(word_counter)}\")\n",
    "print(f\"  Average word frequency: {len(all_words) / len(word_counter):.2f}\")\n",
    "\n",
    "print(f\"\\nüèÜ Top 10 most frequent words:\")\n",
    "for word, count in word_counter.most_common(10):\n",
    "    print(f\"  '{word}': {count}\")\n",
    "\n",
    "# Sentence type analysis (based on ending punctuation)\n",
    "sentence_types = {'statement': 0, 'question': 0, 'exclamation': 0, 'other': 0}\n",
    "for text in punctuated_texts:\n",
    "    if text.endswith('‡•§') or text.endswith('.'):\n",
    "        sentence_types['statement'] += 1\n",
    "    elif text.endswith('?'):\n",
    "        sentence_types['question'] += 1\n",
    "    elif text.endswith('!'):\n",
    "        sentence_types['exclamation'] += 1\n",
    "    else:\n",
    "        sentence_types['other'] += 1\n",
    "\n",
    "print(f\"\\nüìÑ Sentence type distribution:\")\n",
    "for stype, count in sentence_types.items():\n",
    "    percentage = (count / len(punctuated_texts)) * 100\n",
    "    print(f\"  {stype.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize text characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Character frequency (top 15)\n",
    "top_chars = dict(char_counter.most_common(15))\n",
    "axes[0, 0].bar(range(len(top_chars)), list(top_chars.values()), color='lightgreen')\n",
    "axes[0, 0].set_xticks(range(len(top_chars)))\n",
    "axes[0, 0].set_xticklabels(list(top_chars.keys()), rotation=45)\n",
    "axes[0, 0].set_xlabel('Bangla Characters')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Top 15 Bangla Character Frequency')\n",
    "\n",
    "# Word length distribution\n",
    "word_lengths = [len(word) for word in all_words]\n",
    "axes[0, 1].hist(word_lengths, bins=15, alpha=0.7, color='lightpink')\n",
    "axes[0, 1].set_xlabel('Word Length (characters)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Word Length Distribution')\n",
    "\n",
    "# Sentence type pie chart\n",
    "axes[1, 0].pie(sentence_types.values(), labels=sentence_types.keys(), \n",
    "               autopct='%1.1f%%', startangle=90)\n",
    "axes[1, 0].set_title('Sentence Type Distribution')\n",
    "\n",
    "# Vocabulary richness over time (if we had temporal data)\n",
    "# For now, show cumulative unique words\n",
    "unique_words_cumulative = []\n",
    "seen_words = set()\n",
    "for i, text in enumerate(original_texts):\n",
    "    words = text.split()\n",
    "    seen_words.update(words)\n",
    "    unique_words_cumulative.append(len(seen_words))\n",
    "\n",
    "axes[1, 1].plot(unique_words_cumulative, color='purple')\n",
    "axes[1, 1].set_xlabel('Sample Index')\n",
    "axes[1, 1].set_ylabel('Cumulative Unique Words')\n",
    "axes[1, 1].set_title('Vocabulary Growth')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adversarial Examples Exploration\n",
    "print(\"‚öîÔ∏è ADVERSARIAL EXAMPLES ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize adversarial attack generator\n",
    "adversarial = AdversarialAttacks()\n",
    "\n",
    "# Generate different types of adversarial examples\n",
    "sample_original = \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶•‡¶æ‡¶ï‡¶ø\"\n",
    "sample_punctuated = \"‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá ‡¶•‡¶æ‡¶ï‡¶ø‡•§\"\n",
    "\n",
    "print(\"üéØ Original sample:\")\n",
    "print(f\"  Without punctuation: {sample_original}\")\n",
    "print(f\"  With punctuation: {sample_punctuated}\")\n",
    "\n",
    "print(\"\\nüîÑ Generating adversarial examples...\")\n",
    "\n",
    "# Character-level attacks\n",
    "char_attacks = []\n",
    "try:\n",
    "    char_sub = adversarial.character_substitution(sample_original)\n",
    "    char_attacks.append((\"Character Substitution\", char_sub))\n",
    "    \n",
    "    char_del = adversarial.character_deletion(sample_original)\n",
    "    char_attacks.append((\"Character Deletion\", char_del))\n",
    "    \n",
    "    char_ins = adversarial.character_insertion(sample_original)\n",
    "    char_attacks.append((\"Character Insertion\", char_ins))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error in character attacks: {e}\")\n",
    "\n",
    "# Word-level attacks  \n",
    "word_attacks = []\n",
    "try:\n",
    "    word_sub = adversarial.word_substitution(sample_original)\n",
    "    word_attacks.append((\"Word Substitution\", word_sub))\n",
    "    \n",
    "    word_swap = adversarial.word_swap(sample_original)\n",
    "    word_attacks.append((\"Word Swap\", word_swap))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error in word attacks: {e}\")\n",
    "\n",
    "# Noise attacks\n",
    "noise_attacks = []\n",
    "try:\n",
    "    space_noise = adversarial.add_spacing_noise(sample_original)\n",
    "    noise_attacks.append((\"Spacing Noise\", space_noise))\n",
    "    \n",
    "    case_noise = adversarial.add_case_noise(sample_original)\n",
    "    noise_attacks.append((\"Case Noise\", case_noise))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error in noise attacks: {e}\")\n",
    "\n",
    "# Display attacks\n",
    "all_attacks = char_attacks + word_attacks + noise_attacks\n",
    "print(f\"\\nüé™ Generated {len(all_attacks)} adversarial examples:\")\n",
    "\n",
    "for i, (attack_type, attacked_text) in enumerate(all_attacks, 1):\n",
    "    print(f\"\\n{i}. {attack_type}:\")\n",
    "    print(f\"   Original: {sample_original}\")\n",
    "    print(f\"   Attacked: {attacked_text}\")\n",
    "    \n",
    "    # Calculate edit distance\n",
    "    def edit_distance(s1, s2):\n",
    "        if len(s1) < len(s2):\n",
    "            return edit_distance(s2, s1)\n",
    "        if len(s2) == 0:\n",
    "            return len(s1)\n",
    "        \n",
    "        previous_row = list(range(len(s2) + 1))\n",
    "        for i, c1 in enumerate(s1):\n",
    "            current_row = [i + 1]\n",
    "            for j, c2 in enumerate(s2):\n",
    "                insertions = previous_row[j + 1] + 1\n",
    "                deletions = current_row[j] + 1\n",
    "                substitutions = previous_row[j] + (c1 != c2)\n",
    "                current_row.append(min(insertions, deletions, substitutions))\n",
    "            previous_row = current_row\n",
    "        return previous_row[-1]\n",
    "    \n",
    "    edit_dist = edit_distance(sample_original, attacked_text)\n",
    "    print(f\"   Edit Distance: {edit_dist}\")\n",
    "\n",
    "# Generate adversarial examples for multiple samples\n",
    "print(f\"\\nüìä Generating adversarial examples for dataset analysis...\")\n",
    "adversarial_samples = []\n",
    "\n",
    "for i, (orig, punct) in enumerate(dataset[:5]):  # Analyze first 5 samples\n",
    "    try:\n",
    "        # Apply different attack types\n",
    "        char_sub = adversarial.character_substitution(orig)\n",
    "        word_swap = adversarial.word_swap(orig)\n",
    "        noise = adversarial.add_spacing_noise(orig)\n",
    "        \n",
    "        adversarial_samples.extend([\n",
    "            (orig, char_sub, \"char_substitution\"),\n",
    "            (orig, word_swap, \"word_swap\"),\n",
    "            (orig, noise, \"spacing_noise\")\n",
    "        ])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error processing sample {i}: {e}\")\n",
    "\n",
    "# Analyze adversarial impact\n",
    "if adversarial_samples:\n",
    "    edit_distances = []\n",
    "    attack_types = []\n",
    "    \n",
    "    for orig, adv, attack_type in adversarial_samples:\n",
    "        edit_dist = edit_distance(orig, adv)\n",
    "        edit_distances.append(edit_dist)\n",
    "        attack_types.append(attack_type)\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    adv_df = pd.DataFrame({\n",
    "        'Attack_Type': attack_types,\n",
    "        'Edit_Distance': edit_distances\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìà Adversarial attack impact analysis:\")\n",
    "    print(adv_df.groupby('Attack_Type')['Edit_Distance'].describe())\n",
    "    \n",
    "    # Visualize adversarial impact\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.boxplot(data=adv_df, x='Attack_Type', y='Edit_Distance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Edit Distance by Attack Type')\n",
    "    plt.ylabel('Edit Distance')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(edit_distances, bins=10, alpha=0.7, color='red')\n",
    "    plt.xlabel('Edit Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Adversarial Edit Distances')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No adversarial samples generated for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d22e7",
   "metadata": {},
   "source": [
    "## üìã Summary and Recommendations\n",
    "\n",
    "Based on our exploration of the Bangla punctuation restoration dataset, here are the key findings and recommendations:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Characteristics:**\n",
    "   - The dataset contains varied sentence lengths and structures\n",
    "   - Common punctuation marks include ‡¶¶‡¶æ‡¶Å‡¶°‡¶º‡¶ø (‡•§), comma (,), and question mark (?)\n",
    "   - Most sentences are statements, followed by questions\n",
    "\n",
    "2. **Text Patterns:**\n",
    "   - Bangla text shows rich character diversity within the Unicode range\n",
    "   - Word lengths vary significantly across samples\n",
    "   - Vocabulary shows good diversity for training robust models\n",
    "\n",
    "3. **Adversarial Robustness:**\n",
    "   - Character-level attacks can significantly alter text appearance\n",
    "   - Word-level attacks preserve semantic meaning better\n",
    "   - Models need to be robust against various noise types\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Implement robust text cleaning for Unicode normalization\n",
    "   - Handle edge cases with mixed punctuation styles\n",
    "   - Consider augmentation strategies for minority punctuation types\n",
    "\n",
    "2. **Model Training:**\n",
    "   - Use balanced sampling for different sentence types\n",
    "   - Implement adversarial training for robustness\n",
    "   - Consider multi-task learning with related NLP tasks\n",
    "\n",
    "3. **Evaluation:**\n",
    "   - Test on adversarial examples during development\n",
    "   - Evaluate punctuation-specific metrics\n",
    "   - Monitor performance across different text lengths\n",
    "\n",
    "4. **Future Work:**\n",
    "   - Collect more diverse data sources\n",
    "   - Implement domain adaptation techniques\n",
    "   - Explore transformer-based architectures optimized for Bangla"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
