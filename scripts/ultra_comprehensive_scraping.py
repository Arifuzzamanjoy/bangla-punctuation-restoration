#!/usr/bin/env python3
"""
ULTRA-COMPREHENSIVE Bangla Data Collection from the ENTIRE INTERNET
This script demonstrates accessing ALL possible Bangla sources on the internet
"""

import sys
import os
import argparse
import logging
from datetime import datetime
import json

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.data.web_scraper import BanglaWebScraper
from config import GENERATION_CONFIG

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'ultra_comprehensive_scraping_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def show_all_available_sources():
    """Display ALL available Bangla sources that can be scraped"""
    
    scraper = BanglaWebScraper()
    
    print("ğŸŒ ULTRA-COMPREHENSIVE BANGLA DATA SOURCES")
    print("=" * 80)
    print()
    
    total_sources = 0
    
    for category, sources in scraper.bangla_sources.items():
        print(f"ğŸ“‚ {category.upper().replace('_', ' ')} ({len(sources)} sources)")
        total_sources += len(sources)
        
        # Show first few sources as examples
        for i, source in enumerate(sources[:5]):
            print(f"   ğŸ”¸ {source}")
        
        if len(sources) > 5:
            print(f"   ... and {len(sources) - 5} more sources")
        print()
    
    print(f"ğŸ“¡ RSS FEEDS ({len(scraper.rss_feeds)} feeds)")
    for i, feed in enumerate(scraper.rss_feeds[:5]):
        print(f"   ğŸ”¸ {feed}")
    if len(scraper.rss_feeds) > 5:
        print(f"   ... and {len(scraper.rss_feeds) - 5} more RSS feeds")
    print()
    
    print(f"ğŸ” SEARCH PATTERNS ({len(scraper.search_patterns)} patterns)")
    for pattern in scraper.search_patterns:
        print(f"   ğŸ”¸ {pattern}")
    print()
    
    total_sources += len(scraper.rss_feeds) + len(scraper.search_patterns)
    
    print("ğŸ¯ ADDITIONAL DATA SOURCES:")
    print("   ğŸ“š Internet Archive (Historical content)")
    print("   ğŸ“± Social Media Platforms (Reddit, Quora, etc.)")
    print("   ğŸ“ Academic Repositories (ResearchGate, Google Scholar, etc.)")
    print("   ğŸ›ï¸ Government Documents and Portals")
    print("   ğŸ¥ Multimedia Transcripts (YouTube, streaming platforms)")
    print("   ğŸ” Search Engine Discovery (Google, Bing, DuckDuckGo)")
    print()
    
    print(f"ğŸ“Š TOTAL ACCESSIBLE SOURCES: {total_sources}+ sources")
    print("   ğŸ’¡ Plus unlimited discovery through search engines!")
    print()
    
    print("âœ¨ SCRAPING CAPABILITIES:")
    print("   ğŸ¤– Automatic Bangla text detection")
    print("   ğŸ§¹ Quality filtering and deduplication")
    print("   â±ï¸ Respectful rate limiting")
    print("   ğŸ”„ Error handling and retry mechanisms")
    print("   ğŸ“Š Real-time progress tracking")
    print("   ğŸ’¾ Automatic data saving")
    print()

def demonstrate_maximum_scraping():
    """Demonstrate maximum internet scraping capabilities"""
    
    logger.info("ğŸš€ ACTIVATING MAXIMUM INTERNET SCRAPING MODE")
    logger.info("=" * 80)
    
    scraper = BanglaWebScraper()
    
    # Show what we're about to scrape
    logger.info("ğŸ“‹ SOURCES TO BE SCRAPED:")
    
    source_count = 0
    for category, sources in scraper.bangla_sources.items():
        logger.info(f"   ğŸ“‚ {category}: {len(sources)} sources")
        source_count += len(sources)
    
    logger.info(f"   ğŸ“¡ RSS feeds: {len(scraper.rss_feeds)} feeds")
    logger.info(f"   ğŸ” Search patterns: {len(scraper.search_patterns)} patterns")
    
    total_potential = source_count + len(scraper.rss_feeds) + len(scraper.search_patterns)
    logger.info(f"   ğŸ¯ TOTAL POTENTIAL SOURCES: {total_potential}+")
    logger.info("")
    
    # Start comprehensive scraping
    logger.info("ğŸŒ STARTING MAXIMUM INTERNET SCRAPING...")
    
    # Use the ultra-comprehensive method
    all_sentences = scraper.scrape_comprehensive_bangla_data(
        wikipedia_articles=300,  # More Wikipedia articles
        news_articles_per_site=30,  # More articles per site
        include_blogs=True,
        include_educational=True,
        include_all_internet=True  # THIS ACTIVATES MAXIMUM SCRAPING
    )
    
    logger.info(f"ğŸ¯ TOTAL SENTENCES COLLECTED: {len(all_sentences):,}")
    
    # Analysis
    if all_sentences:
        logger.info("ğŸ“Š COLLECTION ANALYSIS:")
        logger.info(f"   ğŸ“ Average sentence length: {sum(len(s.split()) for s in all_sentences) / len(all_sentences):.1f} words")
        
        # Count punctuation types
        punct_count = {}
        for punct in ['à¥¤', '?', '!', ',', ';', ':']:
            count = sum(s.count(punct) for s in all_sentences)
            if count > 0:
                punct_count[punct] = count
        
        logger.info("   ğŸ“Š Punctuation distribution:")
        for punct, count in punct_count.items():
            logger.info(f"      '{punct}': {count:,} occurrences")
        
        # Save comprehensive data
        output_data = {
            "collection_date": datetime.now().isoformat(),
            "total_sentences": len(all_sentences),
            "total_sources_accessed": total_potential,
            "scraping_method": "ULTRA_COMPREHENSIVE_INTERNET_WIDE",
            "sentence_analysis": {
                "avg_length": sum(len(s.split()) for s in all_sentences) / len(all_sentences),
                "punctuation_distribution": punct_count
            },
            "sample_sentences": all_sentences[:20],  # First 20 as samples
            "source_categories": list(scraper.bangla_sources.keys()),
            "capabilities": [
                "Major news portals (40+ sites)",
                "International Bangla news",
                "Literary websites and magazines", 
                "Educational and academic content",
                "Government documents",
                "Blog and forum content",
                "RSS feeds (30+ feeds)",
                "Social media content",
                "Internet Archive historical data",
                "Multimedia transcripts",
                "Search engine discovery"
            ]
        }
        
        with open('ultra_comprehensive_bangla_data.json', 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info("ğŸ’¾ Ultra-comprehensive data saved to 'ultra_comprehensive_bangla_data.json'")
        
        # Show some examples
        logger.info("ğŸ“ SAMPLE COLLECTED SENTENCES:")
        for i, sentence in enumerate(all_sentences[:5]):
            logger.info(f"   {i+1}. {sentence}")
    
    else:
        logger.warning("âš ï¸ No sentences collected - check internet connection and source availability")
    
    return all_sentences

def test_specific_source_category(category: str):
    """Test scraping from a specific source category"""
    
    scraper = BanglaWebScraper()
    
    if category not in scraper.bangla_sources:
        logger.error(f"âŒ Category '{category}' not found!")
        logger.info(f"Available categories: {list(scraper.bangla_sources.keys())}")
        return
    
    sources = scraper.bangla_sources[category]
    logger.info(f"ğŸ¯ Testing {category} category ({len(sources)} sources)")
    
    all_sentences = []
    
    for i, source in enumerate(sources[:3]):  # Test first 3 sources
        logger.info(f"ğŸ“‚ Scraping {i+1}/{min(3, len(sources))}: {source}")
        
        try:
            sentences = scraper.scrape_url(source, "general")
            all_sentences.extend(sentences)
            logger.info(f"   âœ… Collected {len(sentences)} sentences")
            
        except Exception as e:
            logger.error(f"   âŒ Error: {e}")
    
    logger.info(f"ğŸ¯ Total from {category}: {len(all_sentences)} sentences")
    
    if all_sentences:
        logger.info("ğŸ“ Sample sentences:")
        for i, sentence in enumerate(all_sentences[:3]):
            logger.info(f"   {i+1}. {sentence}")

def main():
    parser = argparse.ArgumentParser(description='Ultra-comprehensive Bangla data collection from entire internet')
    parser.add_argument('--show-sources', action='store_true',
                       help='Show all available Bangla sources on the internet')
    parser.add_argument('--maximum-scraping', action='store_true',
                       help='Activate maximum internet scraping mode')
    parser.add_argument('--test-category', type=str,
                       help='Test scraping from specific category')
    parser.add_argument('--list-categories', action='store_true',
                       help='List all available source categories')
    
    args = parser.parse_args()
    
    if args.show_sources:
        show_all_available_sources()
    
    elif args.maximum_scraping:
        logger.info("âš ï¸ WARNING: Maximum scraping will access 100+ sources and may take hours!")
        response = input("Continue? (y/N): ")
        
        if response.lower() == 'y':
            sentences = demonstrate_maximum_scraping()
            logger.info(f"âœ… Maximum scraping completed! Total: {len(sentences):,} sentences")
        else:
            logger.info("âŒ Maximum scraping cancelled by user")
    
    elif args.test_category:
        test_specific_source_category(args.test_category)
    
    elif args.list_categories:
        scraper = BanglaWebScraper()
        logger.info("ğŸ“‚ Available source categories:")
        for category in scraper.bangla_sources.keys():
            logger.info(f"   ğŸ”¸ {category}")
    
    else:
        parser.print_help()
        print("\nğŸ’¡ USAGE EXAMPLES:")
        print("python scripts/ultra_comprehensive_scraping.py --show-sources")
        print("python scripts/ultra_comprehensive_scraping.py --maximum-scraping")
        print("python scripts/ultra_comprehensive_scraping.py --test-category major_news_portals")
        print("python scripts/ultra_comprehensive_scraping.py --list-categories")
        print()
        print("ğŸŒ ACCESS TO ENTIRE INTERNET:")
        print("âœ… 40+ Major Bangla news portals")
        print("âœ… International Bangla news sources")
        print("âœ… Educational and academic websites")
        print("âœ… Government documents and portals")
        print("âœ… Literary websites and magazines")
        print("âœ… Blog and forum content")
        print("âœ… Social media platforms")
        print("âœ… RSS feeds (30+ feeds)")
        print("âœ… Internet Archive historical data")
        print("âœ… Multimedia transcripts")
        print("âœ… Search engine discovery")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ Scraping interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
